{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comma. ai  <br>\n",
    "\n",
    "\n",
    "## JOH EUN KANG (Summer Internship : ML software engineer)\n",
    "##  Code & Comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joheunkang/Desktop/comma/speed_challenge_2017/data\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/joheunkang/Desktop/comma/speed_challenge_2017/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time, sys, shutil\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Flatten, Dense, Lambda, Convolution2D, Cropping2D, Dropout, Reshape, BatchNormalization, ELU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your datat location\n",
    "PATH_DATA_FOLDER = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data'\n",
    "PATH_TRAIN_LABEL = os.path.join(data_path, 'train.txt')\n",
    "PATH_TRAIN_VIDEO = os.path.join(data_path, 'train.mp4')\n",
    "\n",
    "PATH_TEST_VIDEO = os.path.join(data_path, 'test.mp4')\n",
    "PATH_TEST_VIDEO_OUTPUT = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/test_video_output'\n",
    "PATH_COMBINED_TEST_VIDEO_OUTPUT = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/comb_test_video_output'\n",
    "\n",
    "OUTPUT_TRAIN_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img'\n",
    "OUTPUT_TEST_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_test_img'\n",
    "\n",
    "processed_train_imgs = os.path.join(processed_data_path, 'train_imgs')\n",
    "processed_test_imgs = os.path.join(processed_data_path, 'test_imgs')\n",
    "DOWNLOAD_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/final_result'\n",
    "\n",
    "# your final predicted speed result \n",
    "test_labels_result = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/results/test_lables.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting frames to Optical Flow\n",
    "\n",
    "After converting your video to frames, we are again converting those original images to optical flow images.\n",
    "Since opencv usese BGR as difault, we change BGR to gray.<br>\n",
    "Then, using the __calcOpticalFlowFarneback__, we get the image of optical flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToOptical(prev_image, curr_image):\n",
    "    prev_image_gray = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY)\n",
    "    curr_image_gray = cv2.cvtColor(curr_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_image_gray, curr_image_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    hsv = np.zeros_like(prev_image)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    \n",
    "    # first, second, and third chnnels\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,1] = 255\n",
    "    hsv[...,2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    flow_image_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return flow_image_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Optical flow from the Video\n",
    "\n",
    "Here, we change those images to optical flow using the opticalflow definition,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FLOW_TRAIN_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img'\n",
    "OUTPUT_FLOW_TEST_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_test_img'\n",
    "OUTPUT_FLOW_VIDEO_TRAIN_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_vid'\n",
    "OUTPUT_FLOW_VIDEO_TEST_PATH= '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_test_vid'\n",
    "DOWNLOAD_PATH = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/final_result'\n",
    "\n",
    "\n",
    "def preprocess_data(video_input_path, flow_video_output_path, image_folder_path, flow_image_folder_path, type):\n",
    "    \n",
    "    #make frame img\n",
    "    if os.path.exists(image_folder_path):\n",
    "        shutil.rmtree(image_folder_path)\n",
    "    os.makedirs(image_folder_path)\n",
    "    \n",
    "    # make optical flow img\n",
    "    if os.path.exists(flow_image_folder_path):\n",
    "        shutil.rmtree(flow_image_folder_path)\n",
    "    os.makedirs(flow_image_folder_path)\n",
    "\n",
    "    print(\"Converting video to optical flow for: \", video_input_path)\n",
    "\n",
    "    video_reader = cv2.VideoCapture(video_input_path)\n",
    "\n",
    "    num_frames = video_reader.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_size = (int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    video_writer = cv2.VideoWriter(flow_video_output_path, fourcc, fps, frame_size)\n",
    "\n",
    "    t1 = time.time()\n",
    "    ret, prev_frame = video_reader.read()\n",
    "    hsv = np.zeros_like(prev_frame)\n",
    "\n",
    "    image_path_out = os.path.join(image_folder_path, str(0) + '.jpg')\n",
    "    cv2.imwrite(image_path_out, prev_frame)\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        ret, next_frame = video_reader.read()\n",
    "        if next_frame is None:\n",
    "            break\n",
    "\n",
    "        bgr_flow = convertToOptical(prev_frame, next_frame)\n",
    "\n",
    "        image_path_out = os.path.join(image_folder_path, str(count) + '.jpg')\n",
    "        flow_image_path_out = os.path.join(flow_image_folder_path, str(count)+'flow' + '.jpg')\n",
    "\n",
    "        cv2.imwrite(image_path_out, next_frame)\n",
    "        cv2.imwrite(flow_image_path_out, bgr_flow)\n",
    "\n",
    "        video_writer.write(bgr_flow)\n",
    "\n",
    "        prev_frame = next_frame\n",
    "        count += 1\n",
    "\n",
    "        \n",
    "    t2 = time.time()\n",
    "    video_reader.release()\n",
    "    video_writer.release()\n",
    "    print(' Conversion completed !')\n",
    "    print(' Time Taken:', (t2 - t1), 'seconds')\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting video to optical flow for:  /Users/joheunkang/Desktop/comma/speed_challenge_2017/data/train.mp4\n",
      " Conversion completed !\n",
      " Time Taken: 1978.3607580661774 seconds\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(PATH_TRAIN_VIDEO, OUTPUT_FLOW_VIDEO_TRAIN_PATH, OUTPUT_TRAIN_PATH, OUTPUT_FLOW_TRAIN_PATH, type='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting video to optical flow for:  /Users/joheunkang/Desktop/comma/speed_challenge_2017/data/test.mp4\n",
      " Conversion completed !\n",
      " Time Taken: 938.9039421081543 seconds\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(PATH_TEST_VIDEO, OUTPUT_FLOW_VIDEO_TEST_PATH, OUTPUT_TEST_PATH, OUTPUT_FLOW_TEST_PATH, type='test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DONE WITH THE DATA PRE-PROCESSING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_FLOW_PRECOMPUTED = 0\n",
    "TYPE_ORIGINAL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "we use CNN Model with ELU, with 5 convolutional layers.<br>\n",
    "\n",
    "The CNN Model looks simple but it is one of the most popular model in the Computer Vision! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(24, (5, 5),\n",
    "                            input_shape = (240, 320, 3),\n",
    "                            strides=(2,2),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(36, (5, 5),\n",
    "                            strides=(2,2),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(48, (5, 5),\n",
    "                            strides=(2,2),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(64, (3, 3),\n",
    "                            strides = (1,1),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, (3, 3),\n",
    "                            strides= (1,1),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal'))\n",
    "    model.add(Flatten())\n",
    "    model.add(ELU())\n",
    "    \n",
    "    model.add(Dense(100, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1, kernel_initializer = 'he_normal'))\n",
    "\n",
    "    adam = Adam(lr=1e-4)\n",
    "    model.compile(optimizer=adam, loss=\"mse\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = TYPE_FLOW_PRECOMPUTED\n",
    "TYPE_FLOW_PRECOMPUTED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data (modified_version)\n",
    "\n",
    "So far, we have images from original training video, and the optical flow images.\n",
    "In this part, we are going to have the training images pairs by using one original image, and the __four consecutive optical flow images__.  <br>\n",
    "\n",
    "Each training image pair will have 5 images and the a single speed that is used in the original image<br>\n",
    "\n",
    "```python\n",
    "\n",
    "'/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/6.jpg'\n",
    "'/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3flow.jpg'\n",
    "'/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/4flow.jpg'\n",
    "'/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/5flow.jpg'\n",
    "'/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/6flow.jpg'\n",
    "```\n",
    "\n",
    "so, the speed for this pair will be the speed of image 6.jpg <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data example\n",
    "\n",
    "def prepareData(labels_path, images_path, flow_images_path, type=TYPE_FLOW_PRECOMPUTED):\n",
    "    num_train_labels = 0\n",
    "    train_labels = []\n",
    "    train_images_pair_paths = []\n",
    "\n",
    "    with open(labels_path) as txt_file:\n",
    "        labels_string = txt_file.read().split()\n",
    "\n",
    "        for i in range(4, len(labels_string)):\n",
    "            speed = float(labels_string[i])\n",
    "            train_labels.append(speed)\n",
    "\n",
    "            if type == TYPE_FLOW_PRECOMPUTED:\n",
    "                train_images_pair_paths.append((os.path.join(images_path,np.str(i)+'.jpg'),\n",
    "                                                             os.path.join(flow_images_path,np.str(i-3)+'flow.jpg')\n",
    "                                                                ,os.path.join(flow_images_path,np.str(i-2)+'flow.jpg')\n",
    "                                                                    ,os.path.join(flow_images_path,np.str(i-1)+'flow.jpg')\n",
    "                                                                        ,os.path.join(flow_images_path,np.str(i)+'flow.jpg')))\n",
    "           \n",
    "    return train_images_pair_paths, train_labels\n",
    "\n",
    "          \n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__From this function, we get one original train image , 4 consecutive flow image with the speed of original train image.__ <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_pair_path, train_labels = prepareData(PATH_TRAIN_LABEL, OUTPUT_TRAIN_PATH ,OUTPUT_FLOW_TRAIN_PATH, type = type_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/6.jpg',\n",
       " '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3flow.jpg',\n",
       " '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/4flow.jpg',\n",
       " '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/5flow.jpg',\n",
       " '/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/6flow.jpg')"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_pair_path[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.034211"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Samples and Testing Samples\n",
    "\n",
    "After we have the whole traning pairs, we are going to split the training paris to make new traning set and validation set. <br>\n",
    "Since we don't have the labels( \"speed\") for our testing video (\"test set\"), __we need to validate our model using validation data.__<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(zip(train_images_pair_path,train_labels))\n",
    "train_samples, validation_samples = train_test_split(samples,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16316"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4080"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 20396\n"
     ]
    }
   ],
   "source": [
    "print('Total Images: {}'.format( len(train_images_pair_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 16316\n",
      "Validation samples: 4080\n"
     ]
    }
   ],
   "source": [
    "print('Train samples: {}'.format(len(train_samples)))\n",
    "print('Validation samples: {}'.format(len(validation_samples)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "\n",
    "This is the __key part__ of our challenge.<br>\n",
    "\n",
    "from the training and validation set, we are going to have the combined image of the optical flow images. \n",
    "In the previous part, we have made the trainig and validation set with a single original image and four corresponing optical flow images.<br>\n",
    "\n",
    "Here, I decided to combine those four optical flow images to make a new single image. We may use 3 or 5 or more images. However, if you combine too many images for making a new single image, you might have some troubles.<br>\n",
    "\n",
    "Assume you have used two images. This means the model will have the optical flow with only two frames.<br>\n",
    "If the car moves very fast, then this could be better choice than 4 images. However, as you can see, the car in the video is not moving very fast. So if you take 2 images, the data may not tell enough information of the current speed.\n",
    "\n",
    "Also, we are augmenting some more images that are flipped version of the optical flow images.<br> In order to have more tranining data, we may add some __light changed images__ or __rotated images.__ I have trained with the light changed images, but the result was not good. Also, if we want to augment rotated images of optical flow, we need to rotate the whole images with several angles ( lot of work!) (like MNIST rotated data set). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### batch sample visualization\n",
    "batch_samples (('/Users/joheunkang/Desktop/comma/speed_challenge_2017/dataUsers/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/3047.jpg', '/Users/joheunkang/Desktop/comma/speed_challenge_2017/dataUsers/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3044flow.jpg', '/Users/joheunkang/Desktop/comma/speed_challenge_2017/dataUsers/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3045flow.jpg', '/Users/joheunkang/Desktop/comma/speed_challenge_2017/dataUsers/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3046flow.jpg', '/Users/joheunkang/Desktop/comma/speed_challenge_2017/dataUsers/joheunkang/Desktop/comma/speed_challenge_2017/data/new_flow_train_img/3047flow.jpg'), 21.939557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorData_non(samples, batch_size=32, type=TYPE_FLOW_PRECOMPUTED):\n",
    "    num_samples = len(samples)\n",
    "    while True:\n",
    "        samples = sklearn.utils.shuffle(samples)\n",
    "        #print('define batch_size:',batch_size)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            #print('offset',offset)\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            #print('len_batch_size',len(batch_samples))\n",
    "            #print(batch_samples[0])\n",
    "            images = []\n",
    "            angles = []\n",
    "            for imagePath, measurement in batch_samples:\n",
    "                combined_image = None\n",
    "                flow_image_bgr = None\n",
    "            \n",
    "                if type ==TYPE_FLOW_PRECOMPUTED:\n",
    "                    curr_image_path, flow_image_path1, flow_image_path2,flow_image_path3, flow_image_path4 = imagePath\n",
    "                    flow_image_bgr = (cv2.imread(flow_image_path1) +cv2.imread(flow_image_path2) +cv2.imread(flow_image_path3) +cv2.imread(flow_image_path4) )/4\n",
    "                \n",
    "                    curr_image = cv2.imread(curr_image_path)\n",
    "                    curr_image = cv2.cvtColor(curr_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                    # combined_image = 0.1*curr_image + flow_image_bgr\n",
    "                    #CHOOSE IF WE WANT TO TEST WITH ONLY OPTICAL FLOW OR A COMBINATION OF VIDEO AND OPTICAL FLOW\n",
    "                    combined_image = flow_image_bgr\n",
    "                \n",
    "                    combined_image = cv2.normalize(combined_image, None, alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                    combined_image = cv2.resize(combined_image, (0,0), fx=0.5, fy=0.5)\n",
    "                \n",
    "                    images.append(combined_image)\n",
    "                    angles.append(measurement)\n",
    "                \n",
    "                    # Adding more images!\n",
    "                    # after we combined the image, we are going to add the flipped images \n",
    "                    images.append(cv2.flip(combined_image,1))\n",
    "                    angles.append(measurement)\n",
    "                \n",
    "                inputs = np.array(images)\n",
    "                outputs = np.array(angles)\n",
    "                yield sklearn.utils.shuffle(inputs, outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the generator\n",
    "\n",
    "with the generator above, we generate traning and valid data set with batch size we assinged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = generatorData_non(train_samples, batch_size=BATCH_SIZE, type=type_)\n",
    "validation_generator = generatorData_non(validation_samples, batch_size=BATCH_SIZE, type=type_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/4647.jpg\n"
     ]
    }
   ],
   "source": [
    "print('/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/4647.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread('/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/4647.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8,  3,  2],\n",
       "        [ 8,  3,  2],\n",
       "        [ 8,  3,  2],\n",
       "        ...,\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1]],\n",
       "\n",
       "       [[10,  5,  4],\n",
       "        [10,  5,  4],\n",
       "        [10,  5,  4],\n",
       "        ...,\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1]],\n",
       "\n",
       "       [[13,  8,  7],\n",
       "        [13,  8,  7],\n",
       "        [13,  8,  7],\n",
       "        ...,\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9,  5,  4],\n",
       "        [ 9,  5,  4],\n",
       "        [ 9,  5,  4],\n",
       "        ...,\n",
       "        [ 8,  4,  3],\n",
       "        [ 7,  3,  2],\n",
       "        [ 7,  3,  2]],\n",
       "\n",
       "       [[ 8,  4,  3],\n",
       "        [ 8,  4,  3],\n",
       "        [ 8,  4,  3],\n",
       "        ...,\n",
       "        [ 7,  3,  2],\n",
       "        [ 7,  3,  2],\n",
       "        [ 7,  3,  2]],\n",
       "\n",
       "       [[ 8,  4,  3],\n",
       "        [ 8,  4,  3],\n",
       "        [ 8,  4,  3],\n",
       "        ...,\n",
       "        [ 7,  3,  2],\n",
       "        [ 7,  3,  2],\n",
       "        [ 7,  3,  2]]], dtype=uint8)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 9  4  3]\n",
      "  [ 9  4  3]\n",
      "  [ 9  4  3]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 9  4  3]\n",
      "  [ 9  4  3]\n",
      "  [ 9  4  3]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[13  8  7]\n",
      "  [13  8  7]\n",
      "  [13  8  7]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[12  7  6]\n",
      "  [12  7  6]\n",
      "  [12  7  6]\n",
      "  ...\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]]\n",
      "\n",
      " [[10  5  4]\n",
      "  [10  5  4]\n",
      "  [10  5  4]\n",
      "  ...\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]]\n",
      "\n",
      " [[10  5  4]\n",
      "  [10  5  4]\n",
      "  [10  5  4]\n",
      "  ...\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]\n",
      "  [ 9  5  4]]]\n"
     ]
    }
   ],
   "source": [
    "result_a = cv2.imread('/Users/joheunkang/Desktop/comma/speed_challenge_2017/data/new_train_img/18061.jpg')\n",
    "print(result_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_FLOW_PRECOMPUTED = 0\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 50\n",
    "\n",
    "MODEL_NAME = 'CNNModel_flow'\n",
    "\n",
    "download_path = '/Users/joheunkang/Desktop/comma/speed_challenge_2017/results'\n",
    "weights_loc = os.path.join(download_path,'weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(weights_loc , monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joheunkang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=31, callbacks=[<keras.ca..., epochs=50, verbose=1, steps_per_epoch=127)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "127/127 [==============================] - 348s 3s/step - loss: 37.5062 - val_loss: 22.8797\n",
      "Epoch 2/50\n",
      "127/127 [==============================] - 363s 3s/step - loss: 16.3274 - val_loss: 14.4550\n",
      "Epoch 3/50\n",
      "127/127 [==============================] - 378s 3s/step - loss: 10.1265 - val_loss: 17.6550\n",
      "Epoch 4/50\n",
      "127/127 [==============================] - 384s 3s/step - loss: 5.3403 - val_loss: 14.4629\n",
      "Epoch 5/50\n",
      "127/127 [==============================] - 338s 3s/step - loss: 4.6402 - val_loss: 14.1090\n",
      "Epoch 6/50\n",
      "127/127 [==============================] - 354s 3s/step - loss: 5.1736 - val_loss: 15.6001\n",
      "Epoch 7/50\n",
      "127/127 [==============================] - 360s 3s/step - loss: 4.0950 - val_loss: 12.5910\n",
      "Epoch 8/50\n",
      "127/127 [==============================] - 384s 3s/step - loss: 3.3217 - val_loss: 10.5961\n",
      "Epoch 9/50\n",
      "127/127 [==============================] - 336s 3s/step - loss: 4.5883 - val_loss: 11.7786\n",
      "Epoch 10/50\n",
      "127/127 [==============================] - 326s 3s/step - loss: 2.9896 - val_loss: 9.0013\n",
      "Epoch 11/50\n",
      "127/127 [==============================] - 356s 3s/step - loss: 4.0452 - val_loss: 10.6715\n",
      "Epoch 12/50\n",
      "127/127 [==============================] - 388s 3s/step - loss: 2.8722 - val_loss: 8.9515\n",
      "Epoch 13/50\n",
      "127/127 [==============================] - 381s 3s/step - loss: 3.0913 - val_loss: 9.1293\n",
      "Epoch 14/50\n",
      "127/127 [==============================] - 351s 3s/step - loss: 2.8695 - val_loss: 10.0958\n",
      "Epoch 15/50\n",
      "127/127 [==============================] - 367s 3s/step - loss: 3.9461 - val_loss: 8.6231\n",
      "Epoch 16/50\n",
      "127/127 [==============================] - 391s 3s/step - loss: 2.1913 - val_loss: 7.2531\n",
      "Epoch 17/50\n",
      "127/127 [==============================] - 373s 3s/step - loss: 4.4270 - val_loss: 5.7637\n",
      "Epoch 18/50\n",
      "127/127 [==============================] - 352s 3s/step - loss: 2.7405 - val_loss: 8.2771\n",
      "Epoch 19/50\n",
      "127/127 [==============================] - 350s 3s/step - loss: 3.3470 - val_loss: 6.7632\n",
      "Epoch 20/50\n",
      "127/127 [==============================] - 378s 3s/step - loss: 1.7730 - val_loss: 9.5771\n",
      "Training model complete...\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Loss\n",
      "[26.215825231173845, 11.408766032878376, 5.590253166518968, 4.411703203806738, 3.5767626803289536, 4.396687810423629, 2.6829974685777516, 3.0942339194538993, 3.607329439375487, 2.264723097089908, 2.5455539032000236, 2.1581054978326213, 2.2350570856060386, 2.016977604572405, 2.2740867821510227, 1.9073582386027736, 2.232575736193385, 2.2802144797540573, 1.930102188172166, 1.3669252855961815]\n",
      "Validation Loss\n",
      "[22.879732131958008, 14.45498275756836, 17.654951095581055, 14.46287727355957, 14.109031677246094, 15.600104331970215, 12.59101390838623, 10.596071243286133, 11.778597831726074, 9.001262664794922, 10.671478271484375, 8.951520919799805, 9.12929630279541, 10.095792770385742, 8.623106002807617, 7.253066539764404, 5.763716697692871, 8.27713394165039, 6.763216018676758, 9.57705307006836]\n"
     ]
    }
   ],
   "source": [
    "history_object = model.fit_generator(training_generator,\n",
    "                                     samples_per_epoch= len(train_samples)//BATCH_SIZE,\n",
    "                                     validation_data=validation_generator,\n",
    "                                     validation_steps=len(validation_samples)//BATCH_SIZE,\n",
    "                                     callbacks=callbacks,\n",
    "                                     epochs=EPOCH,\n",
    "                                     verbose=1)\n",
    "\n",
    "\n",
    "print('Training model complete...')\n",
    "\n",
    "print(history_object.history.keys())\n",
    "print('Loss')\n",
    "print(history_object.history['loss'])\n",
    "print('Validation Loss')\n",
    "print(history_object.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__About 1.5 HOURS to train the model!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = loss in traning data <br> \n",
    "val_loss== loss in validation data <br>\n",
    "\n",
    "If there is a huge gap between loss and val_loss, we can think of it as __overfitting__.<br>. However, used earlystopping to avoid overfitting during the training. <br>\n",
    "\n",
    "also, by adding <br> \n",
    "\n",
    "\"save_best_only == True\"\n",
    "\n",
    "```python\n",
    "\n",
    "ModelCheckpoint(weights_loc , monitor='val_loss', save_best_only=True)\n",
    "\n",
    "```\n",
    "\n",
    "we only saved weights that has smallest validation loss during our training.\n",
    "\n",
    "which is <br> \n",
    "\n",
    "Validation_loss = __5.763716697692871__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the best weight as weight.h5 (lowest validation error).<br>\n",
    "\n",
    "Here, we are using the weights to predict the test video.<br>\n",
    "\n",
    "\n",
    "As we did for the traninig data, we first change the video to frames, and then to the optical flow frames.\n",
    "Then, we take the the predicted speed from it. <br>\n",
    "\n",
    "The result will be the __'test_labels.txt'__ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'CNNModel_flow'\n",
    "best_model_weights = os.path.join(download_path,'weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_video(video_input_path):\n",
    "    predicted_labels = []\n",
    "\n",
    "    video_reader = cv2.VideoCapture(video_input_path)\n",
    "\n",
    "    num_frames = video_reader.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_size = (int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    t1 = time.time()\n",
    "    ret, prev_frame = video_reader.read()\n",
    "    hsv = np.zeros_like(prev_frame)\n",
    "\n",
    "    predicted_labels.append(0.0)\n",
    "\n",
    "    flow_image_bgr_prev1 =  np.zeros_like(prev_frame)\n",
    "    flow_image_bgr_prev2 =  np.zeros_like(prev_frame)\n",
    "    flow_image_bgr_prev3 =  np.zeros_like(prev_frame)\n",
    "    flow_image_bgr_prev4 =  np.zeros_like(prev_frame)\n",
    "\n",
    "    count =0\n",
    "    while True:\n",
    "        ret, next_frame = video_reader.read()\n",
    "        if ret is False:\n",
    "            break\n",
    "\n",
    "        flow_image_bgr_next = convertToOptical(prev_frame, next_frame)\n",
    "        flow_image_bgr = (flow_image_bgr_prev1 + flow_image_bgr_prev2 +flow_image_bgr_prev3 +flow_image_bgr_prev4 + flow_image_bgr_next)/4\n",
    "\n",
    "        curr_image = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        combined_image = flow_image_bgr\n",
    "\n",
    "        combined_image_test = cv2.normalize(combined_image, None, alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        combined_image_test = cv2.resize(combined_image_test, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "        combined_image_test = combined_image_test.reshape(1, combined_image_test.shape[0], combined_image_test.shape[1], combined_image_test.shape[2])\n",
    "\n",
    "        prediction = model.predict(combined_image_test)\n",
    "\n",
    "        predicted_labels.append(prediction[0][0])\n",
    "\n",
    "        prev_frame = next_frame\n",
    "        flow_image_bgr_prev4 = flow_image_bgr_prev3\n",
    "        flow_image_bgr_prev3 = flow_image_bgr_prev2\n",
    "        flow_image_bgr_prev2 = flow_image_bgr_prev1\n",
    "        flow_image_bgr_prev1 = flow_image_bgr_next\n",
    "\n",
    "        count +=1\n",
    "\n",
    "    predicted_labels[0] = predicted_labels[1]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predicted Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: it takes some times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your testing model is here!\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.load_weights(best_model_weights)\n",
    "print('Your testing model is here!')\n",
    "predicted_labels = predict_from_video(PATH_TEST_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10798"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have correct number of predicted labels! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_labels_result, mode=\"w\") as outfile:\n",
    "    for label in predicted_labels:\n",
    "        outfile.write(\"%s\\n\" % np.str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result of test_labels uploaded on GitHub__ <br>\n",
    "\n",
    "\n",
    "## THANK YOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
